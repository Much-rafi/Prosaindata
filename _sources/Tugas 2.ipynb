{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8uMwVwMI6q6h"],"toc_visible":true,"authorship_tag":"ABX9TyMiMtP3D9fUaBNXJwk4rJ5e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tugas 2"],"metadata":{"id":"41yafl6Z8yPl"}},{"cell_type":"markdown","source":["## Import SQL Server\n","Pada tahapan pertam hal yang harus dilakukan yaitu mengambil data dari SQL server dengann kode tertentu, pada sql server hanya mengambil kolom PETALWIDTH Dan CLASS"],"metadata":{"id":"8uMwVwMI6q6h"}},{"cell_type":"code","source":["# #SQL SERVER\n","# import pyodbc\n","# import pandas as pd\n","\n","# server = 'LAPTOP-IS6U2T23\\SQLEXPRESS01' \n","# database = 'prosain{200411100089}' \n","\n","# cnxn = pyodbc.connect(r'Driver=ODBC Driver 17 for SQL Server;Server=LAPTOP-IS6U2T23\\SQLEXPRESS01;Database=prosain{200411100089};Trusted_Connection=yes;')  \n","# cursor = cnxn.cursor()\n","\n","# # select 26 rows from SQL table to insert in dataframe.\n","# query = \"SELECT petalwidth,class  FROM [prosain{200411100089}].[dbo].[iris.arff]\"\n","# pwcls = pd.read_sql(query, cnxn)\n","\n","# print(pwcls.head())"],"metadata":{"id":"23frIG27C6Lg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Posgre\n","Pada tahapan ini hal yang harus dilakukan yaitu mengambil data dari posgre dengan code tertentu dan memasukan server dan databases sesuai dengan nama masing masing.pada import posgre ini data yang diambil adalah pada kolom sepallength"],"metadata":{"id":"Tnay66Qf7ykB"}},{"cell_type":"code","source":["# import psycopg2\n","\n","# conn = psycopg2.connect(\n","#     database=\"prosain{200411100089}\",\n","#     user=\"postgres\",\n","#     password=\"rafaarekperumaan\",\n","#     host=\"localhost\",\n","#     port=\"5432\"\n","# )\n","\n","# cur = conn.cursor()\n","\n","# cur.execute(\"SELECT iris.sepal_length FROM iris\")\n","\n","# rows = cur.fetchall()\n","\n","# sl = pd.DataFrame(rows, columns=['sepallength'])\n","# sl.head()"],"metadata":{"id":"v5W1ry17DIOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Elephent\n","\n","Pada tahapan ini hal yang harus dilakukan yaitu mengambil data dari elephent dengan code tertentu dengan cara memasukkan nama databases, user, password, host, serta port.pada elephent ini data yang dia,bil adalah pada kolom sepalwidth"],"metadata":{"id":"SfFmExkG8pgl"}},{"cell_type":"code","source":["\n","# import psycopg2\n","\n","# conn = psycopg2.connect(\n","#     database=\"udkxxdqp\",\n","#     user=\"udkxxdqp\",\n","#     password=\"GfyvTprZl6TNXaiRGNU2rJ0g8V0x19F-\",\n","#     host=\"floppy.db.elephantsql.com\",\n","#     port=\"5432\"\n","# )\n","\n","# cur = conn.cursor()\n","\n","# cur.execute(\"SELECT sepalwidth FROM iris\")\n","\n","# rows = cur.fetchall()\n","# sw = pd.DataFrame(rows, columns=['sepalwidth'])\n","# sw.head()"],"metadata":{"id":"SGPiHDCzDluq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import SQL Lokal\n","Pada tahapan ini hal yang arus dilakukam yaitu mengambil data dari sql lokal dengan code tertentu dengan cara memasukkan host,databases, username sesuai dengan nama masing\". Dan data yang diambil adalah petallength"],"metadata":{"id":"NcBYfw3J9RCq"}},{"cell_type":"code","source":["# import mysql.connector\n","\n","# mydb = mysql.connector.connect(\n","#     host=\"localhost\",\n","#     database=\"prosain200411100089\",\n","#     username=\"root\",\n","#     password=\"\"\n","# )\n","\n","# mycursor = mydb.cursor()\n","\n","# mycursor.execute(\"SELECT petallength FROM iris_arff\")\n","\n","# myresult = mycursor.fetchall()\n","\n","# pl = pd.DataFrame(rows,columns=['petallength'])\n","# pl.head()"],"metadata":{"id":"IvuCeu8HDsLb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Penggabungan\n","\n","Setelah mengambil data dari masing masing sumber, selanjutnya digabungkan dengan pd.concat dan memasukkan variable penampung masing\" sumber."],"metadata":{"id":"gVY4rT8i90U_"}},{"cell_type":"code","source":["# iris_df = pd.concat([sl,sw,pl,pwcls],axis=1)\n","# iris_df.head()"],"metadata":{"id":"dvcD8B03Dvse"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import CSV\n","\n","import masing masing data yang telah dipisahkan kedalam csv dengan kode dibawah ini"],"metadata":{"id":"3zQNw8AY-Otn"}},{"cell_type":"code","source":["# # convert to csv\n","# pl.to_csv('pl_iris.csv', index=False)\n","# sl.to_csv('sl_iris.csv', index=False)\n","# sw.to_csv('sw_iris.csv', index=False)\n","# pwcls.to_csv('pwcls_iris.csv', index=False)"],"metadata":{"id":"q0ylDOLgECsl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import pada Orces"],"metadata":{"id":"8z0Th4mwEVoj"}},{"cell_type":"markdown","source":["### Get data"],"metadata":{"id":"-vgDX3N9ElXL"}},{"cell_type":"code","source":["# import orchest\n","# import pandas as pd\n","# # from sklearn import datasets\n","\n","# # Explicitly cache the data in the \"/data\" directory since the\n","# # kernel is running in a Docker container, which are stateless.\n","# # The \"/data\" directory is a special directory managed by Orchest\n","# # to allow data to be persisted and shared across pipelines and\n","# # even projects.\n","# # print(\"Dowloading California housing data...\")\n","# # data = datasets.fetch_california_housing(data_home=\"/data\")\n","# pl=pd.read_csv('dataset/pl_iris.csv')\n","# sl=pd.read_csv('dataset/sl_iris.csv')\n","# sw=pd.read_csv('dataset/sw_iris.csv')\n","# pwcls=pd.read_csv('dataset/pwcls_iris.csv')\n","\n","# data = pd.concat([sl, sw, pl, pwcls], axis=1)\n","\n","# # Convert the data into a DataFrame.\n","# # df_data = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n","# # df_target = pd.DataFrame(data[\"target\"], columns=[\"MedHouseVal\"])\n","# # fitur\n","# df_data = data.iloc[:, :-1]\n","# # target\n","# df_target = data.iloc[:, 4]\n","\n","# # Output the housing data so the next steps can retrieve it.\n","# print(\"Outputting converted \")\n","# orchest.output((df_data, df_target), name=\"data\")\n","# print(\"Success!\")\n","\n","# data.head()"],"metadata":{"id":"XwpfSeEcEZ-L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preposesing / SPLIT"],"metadata":{"id":"wBa0E92sEqXU"}},{"cell_type":"code","source":["import orchest\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","# from sklearn.preprocessing import MinMaxScaler\n","data = orchest.get_inputs() \n","X, y = data[\"data\"]\n","# scaler = MinMaxScaler()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n","# X_train = scaler.fit_transform(X_train)\n","# X_test = scaler.transform(X_test)\n","orchest.output((X_train, y_train, X_test, y_test), name=\"training_data\")"],"metadata":{"id":"PyZd5Ke-EuMd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MODEL"],"metadata":{"id":"7hymc-xzE1Dm"}},{"cell_type":"code","source":["# import numpy as np\n","# import orchest\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.metrics import mean_squared_error\n","# # Retrieve the data from the previous step.\n","# data = orchest.get_inputs()\n","# X_train, y_train, X_test, y_test = data[\"training_data\"]\n","\n","\n","# model = KNeighborsClassifier(n_neighbors = 15)\n","# model.fit(X_train, y_train)\n","# from sklearn.metrics import accuracy_score\n","# y_pred = model.predict(X_test)\n","# test_accracy = accuracy_score(y_test, y_pred)\n","# orchest.output(test_accracy, name=\"KNeighborsClassifier\")"],"metadata":{"id":"8wXyMxM7E_da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import orchest\n","# from sklearn.naive_bayes import GaussianNB\n","# from sklearn.metrics import mean_squared_error\n","# # Retrieve the data from the previous step.\n","# data = orchest.get_inputs()\n","# X_train, y_train, X_test, y_test = data[\"training_data\"]\n","\n","\n","# model = GaussianNB()\n","# model.fit(X_train, y_train)\n","# from sklearn.metrics import accuracy_score\n","# y_pred = model.predict(X_test)\n","# test_accracy = accuracy_score(y_test, y_pred)\n","# orchest.output(test_accracy, name=\"GaussianNB\")"],"metadata":{"id":"eR4pHfcMFLSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import orchest\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.metrics import mean_squared_error\n","# # Retrieve the data from the previous step.\n","# data = orchest.get_inputs()\n","# X_train, y_train, X_test, y_test = data[\"training_data\"]\n","\n","\n","# model = LogisticRegression()\n","# model.fit(X_train, y_train)\n","# from sklearn.metrics import accuracy_score\n","# y_pred = model.predict(X_test)\n","# test_accracy = accuracy_score(y_test, y_pred)\n","# orchest.output(test_accracy, name=\"logistic-regression-accuracy\")"],"metadata":{"id":"GfnwenPQFSet"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hasil"],"metadata":{"id":"uKbuPuKfFe_D"}},{"cell_type":"code","source":["# import orchest\n","# data = orchest.get_inputs()\n","# for name, value in data.items():\n","#     if name != \"unnamed\":\n","#         print(f\"\\n{name:30} {value}\")"],"metadata":{"id":"UTwAFE8JFv3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Penjelasan\n","\n","Setelah kode diatas dimasukan maka akan muncul hasil akurasi pada log run."],"metadata":{"id":"TJkuxXs_G3Yh"}}]}